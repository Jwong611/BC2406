library(data.table)
library(caTools)        # train-test split
library(rpart)          # CART
library(rpart.plot)  
library(car)

setwd("C:/NTU/Y2S1/BC2406 - Predictive Analytics/Projects")

data <- fread('credit_risk_dataset.csv')

# Data Cleaning
#dropping rows with age more than 120 , human lifespan may go up to a max of 120
data <- data[data$person_age < 120 , ]

#dropping rows where their employment years are more than their age
data <- data[data$person_emp_length < data$person_age , ]

#taking out rows that have any NA values 
data <- na.omit(data)


# Factoring Variables
data$person_home_ownership <- as.factor(data$person_home_ownership)
data$loan_intent <- factor(data$loan_intent)
data$cb_person_default_on_file <- as.factor(data$cb_person_default_on_file)
data$loan_status <- factor(data$loan_status,
                           levels = c(0,1),
                           labels = c("Non-Default","Default"))

# Correlation Matrix 
dt <- data

factor_variables <- c("person_home_ownership", "loan_intent", "cb_person_default_on_file", "loan_status")

# Converting factorized variables to numbers for correlation plot
dt[, (factor_variables) := lapply(.SD, as.numeric), .SDcols = factor_variables]

# Plotting a correlation plot
ggcorrplot(round(cor(dt),3), hc.order = TRUE, lab = TRUE) + labs(title= "Correlation Matrix")

#############################################################################################
# Train-Test Split
set.seed(2)

train <- sample.split(data$loan_status , SplitRatio = 0.7)

trainset <- subset(data , train == T)
testset <- subset(data , train ==F)

#############################################################################################

#logistic regression
m1 <- glm(loan_status ~ ., data = trainset , family = binomial)

m1.prob <- predict(m1 , newdata=testset , type = "response")
m1.pred <- ifelse(m1.prob > 0.5 , "Default" , "Non-Default")

table(actual = testset$loan_status ,prediction = m1.pred)

mean(testset$loan_status == m1.pred)

vif(m1)

summary(m1)

# Updated Logistic Model
m2 <- glm(loan_status ~. -person_age -person_income -person_emp_length -cb_person_default_on_file -cb_person_cred_hist_length,
          data = trainset, family = binomial)

m2.prob <- predict(m2 , newdata=testset , type = "response")
m2.pred <- ifelse(m2.prob > 0.5 , "Default" , "Non-Default")

table(actual = testset$loan_status, prediction = m2.pred)

mean(testset$loan_status == m2.pred)
#############################################################################################
# CART Decision Tree
# Maximal Tree
m3 <- rpart(loan_status ~ ., data = trainset, method = "class", 
            control = rpart.control(minsplit = 2, cp = 0))


# Display the pruning sequence and 10-fold CV errors, as a chart.
plotcp(m3, main = "Subtrees in credit risk.csv")

# Extracting the optimal tree
# Compute min CVerror + 1SE in maximal tree.
CVerror.cap <- m3$cptable[which.min(m3$cptable[,"xerror"]), "xerror"] + 
  m3$cptable[which.min(m3$cptable[,"xerror"]), "xstd"]

# Find the optimal CP region whose CV error is just below CVerror.cap in maximal tree cart.
i <- 1; j<- 4
while (m3$cptable[i,j] > CVerror.cap){
  i <- i + 1
}

# Get geometric mean of the two identified CP values in the optimal region if optimal tree has at least one split.
cp.opt = ifelse(i > 1, sqrt(m3$cptable[i,1] * m3$cptable[i-1,1]), 1)

# Prune the tree
m4 <- prune(m3, cp = cp.opt)

m4.prob <- predict(m4 , newdata = testset , type = "prob")

m4.pred <- ifelse(m4.prob[, 2] > 0.5, "Default", "Non-Default")

table(actual = testset$loan_status, prediction = m4pred)

mean(testset$loan_status == m4.pred)

